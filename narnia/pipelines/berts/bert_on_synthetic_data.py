# -*- coding: utf-8 -*-
"""zero-shot-pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1COHYZPOjsy2dd63AGker2YqIIm6_vSAm
"""

!pip install transformers datasets wandb --upgrade

import wandb
import numpy as np
import torch
import json
from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import load_dataset, ClassLabel, load_metric, DatasetDict, concatenate_datasets
import os
import pandas as pd

import wandb

wandb.login()
api = wandb.Api()
artifact = api.artifact('broccoliman/aslan/intent-gpt2:v0')
artifact.download()

artifact = api.artifact('broccoliman/aslan/pretrained-bert-on-seen:v0')
artifact.download()

artifact = api.artifact('broccoliman/aslan/CLINC150-unseen:v1')
artifact.download()

bert = BertForSequenceClassification.from_pretrained("artifacts/pretrained-bert-on-seen:v0").cuda()

os.environ["WANDB_LOG_MODEL"] = "false"

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")
metric = load_metric("accuracy")

config = {
    "per_device_train_batch_size": 32,
    "per_device_eval_batch_size": 64,
    "max_steps": 150,
    "learning_rate": 2e-5,
    "warmup_steps": 0,
    "weight_decay": 0.01,
    "seed": 42,
    "lr_scheduler_type": "linear",
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "gradient_accumulation_steps": 1
}

common_args = {
    "output_dir": "./results",
    "evaluation_strategy": "steps",
    "eval_steps": 10,
    "logging_dir": "./logs",
    "logging_steps": 10,
    "report_to": "wandb",
    "save_strategy": "steps",
    "save_steps": 10,
    "save_total_limit": 5,
    "load_best_model_at_end": True,
    "metric_for_best_model": "accuracy",
    "greater_is_better": True,
    "disable_tqdm": True
}

with open("artifacts/CLINC150-unseen:v1/mapping.json") as f:
    mapping = json.load(f)

from datasets import load_from_disk

dataset = load_from_disk("artifacts/CLINC150-unseen:v1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

def bert_pipeline(bert, support_set, test_set):

    for param in bert.parameters():
        param.requires_grad = True

    num_labels = len(mapping)

    bert.classifier = torch.nn.Linear(768, num_labels)
    bert.classifier.requires_grad = True
    bert.num_labels = num_labels
    bert.config.num_labels = num_labels
    
    run = wandb.init(project="aslan",
                     tags=["bert", "clinc150", "finetuning-on-unseen"],
                     job_type="training",
                     group="short-finetuning-on-unseen",
                     config=config)

    wandb.config["support_size"] = len(support_set) / 38
    run.use_artifact("CLINC150-unseen:latest")
    run.use_artifact("pretrained-bert-on-seen:latest")

    trainer = Trainer(
        model=bert,
        args=TrainingArguments(**config, **common_args), 
        train_dataset=support_set,
        eval_dataset=test_set,
        data_collator=collator,
        compute_metrics=compute_metrics
    )

    trainer.train()

    final_metrics = trainer.evaluate(support_set, metric_key_prefix="train")
    final_metrics.update(trainer.evaluate(test_set, metric_key_prefix="test"))

    wandb.log(final_metrics)

    run.finish()
    return bert

from datasets import concatenate_datasets

def generate_sets(dset, support_size=10):
    full_labels = np.array(dset["label"])

    label_idxs = []

    for i in range(38):
        label_idxs.append(np.where(full_labels == i)[0])

    train, test = [], []
    for i in range(38):
        train_ids = np.random.choice(label_idxs[i], support_size)
        test_ids = list(set(label_idxs[i]) - set(train_ids))
        train.append(dset.select(train_ids))
        test.append(dset.select(test_ids))

    return concatenate_datasets(train).shuffle(), concatenate_datasets(test).shuffle()

train, test = generate_sets()



"""# Здесь происходит GPT-2"""

import wandb
import numpy as np
import torch
import json
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset, ClassLabel, load_metric, DatasetDict
import os
import pandas as pd


# MODEL AND TOKENIZER

tokenizer = GPT2Tokenizer.from_pretrained('gpt2', truncation=True, padding=True)
tokenizer.add_special_tokens({"sep_token": "<sep>", "pad_token": "<pad>", "bos_token": "<start>", "eos_token": "<end>", "unk_token": "<unk>"})
collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

model = GPT2LMHeadModel.from_pretrained("artifacts/intent-gpt2:v0").cuda()
model.resize_token_embeddings(len(tokenizer))

intent = "insurance"

def generate_utterance(intent):
    prompt = "<start>" + intent + "<sep>"
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to('cuda')

    generated = model.generate(
        input_ids,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        max_length=50,
        do_sample=True,
        temperature=.8
    )

    result = tokenizer.decode(generated[0], skip_special_tokens=False)
    return result.split("<sep>")[1].replace("<end>", "").strip()

from tqdm.notebook import tqdm

for intent in tqdm(mapping * 40):
    text = generate_utterance(intent)
    print(intent, text)
    inputs.append((intent, text))

with open("generated_inputs.json", "w") as f:
    json.dump(inputs, f)

"""# Дальше обучаем берт на синтетических данных"""

from datasets import Dataset

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
classlabel = ClassLabel(names=mapping)

intents = [el[0] for el in inputs]
texts = [el[1] for el in inputs]
synthetic = Dataset.from_dict({"intent": intents, "text": texts})

def encode_label(x):
    x["label"] = classlabel.str2int(x["intent"])
    return x

synthetic = synthetic.map(lambda examples: tokenizer(examples["text"]), batched=True)
synthetic = synthetic.map(encode_label)
synthetic.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])

train_syn, _ = generate_sets(synthetic, 10)

bert = bert_pipeline(bert, train_syn, dataset)
